# ğŸ§  AI Flashcard Generator

This project generates study flashcards from educational text using the powerful **Mistral-7B-Instruct** language model. It features a user-friendly **Streamlit interface** and uses **Google Colab** to run heavy model inference. Generated flashcards are automatically saved to a file and can be viewed in the interface.

---

## ğŸ“¦ Features

- Upload `.txt` or `.pdf` files OR paste raw text
- Uses Mistral-7B-Instruct for high-quality Q&A generation
- Generates 5â€“10 flashcards per input (adjustable)
- Saves flashcards to a text file
- Web-based interface built using Streamlit + Pyngrok
- Model runs on Colab backend (lightweight frontend on local device)

---

## ğŸš€ How It Works

1. **Launch Streamlit App**:
   - Accepts user input (file or text)
   - Saves input text as `input.txt` to Colab file system

2. **Colab Model Inference**:
   - Mistral model loads input from `input.txt`
   - Generates flashcards and saves output to `output.txt`

3. **Streamlit Output**:
   - Reads from `output.txt` and displays the flashcards
   - Allows download of the generated flashcards

---

## ğŸ›  Setup Instructions

### ğŸ”¹ 1. Clone the Repository

```bash
git clone https://github.com/yashtijil/LLM-Powered-Flashcard-Generator
cd LLM-Powered-Flashcard-Generator
```

### ğŸ”¹ 2. Install Dependencies
Install required packages using pip:
```bash
pip install -r requirements.txt
```

### ğŸ”¹ 3. Launch the Streamlit App (Local)
```
streamlit run app.py
```
This will start a local server. If you're using Colab, the Streamlit app will automatically be tunneled via Pyngrok and the public URL will be shown.


### ğŸ”¹ 4. Run the Model in Colab
- Open the provided Colab notebook: model_runner.ipynb
- Click Run All after input has been submitted in Streamlit
- The model reads input.txt, generates flashcards, and writes to output.txt
- Return to the Streamlit app to view/download results

### ğŸ”¹ 5. ğŸ“ Project Structure
```text
flashcard-generator/
â”‚
â”œâ”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ LLM_Flashcard.ipynb   # Colab notebook to run the model
â”œâ”€â”€ requirements.txt      # All dependencies
â”œâ”€â”€ sample_input.txt      # Auto-saved input file
â”œâ”€â”€ sample_output.txt     # Generated flashcards
â””â”€â”€ README.md             # Project documentation
```
### ğŸ”¹ 6. ğŸ” Model Details
**Model Used**: [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)

Inference Backend: Google Colab GPU

Memory Efficient: Loaded with 4-bit quantization via bitsandbytes
